{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import string\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk import word_tokenize,sent_tokenize\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "with open('C:\\\\Python\\\\train_90.txt',encoding=\"utf8\",mode=\"r\") as InpFile:\n",
    "    data=InpFile.readlines()\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "stp_wrd=set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('C:\\\\Python\\\\train_90.txt', header=None,names= [\"ItemID\", \"Sentiment\", \"SentimentSource\", \"SentimentText\"],\n",
    "                 skiprows=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "l1=df['SentimentText']\n",
    "ytrain=df['Sentiment']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Cleaning------------->>>>>>>>>>>>>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SentTokenize(data):\n",
    "    tweettknzr = TweetTokenizer(strip_handles=True)\n",
    "    l2=list()\n",
    "    for i in list(range(0,len(data))):\n",
    "        l2.append(tweettknzr.tokenize(''.join(data[i]))) # Tokenized the words at row level\n",
    "    return(l2)\n",
    "\n",
    "def CleanTokenize(data):\n",
    "    TokenizeClean=list()\n",
    "    for i in list(range(0,len(data))):\n",
    "        temp=list()\n",
    "        for j in list(range(len(data[i]))):\n",
    "            if(data[i][j].isalpha() or data[i][j].isnumeric()): # Removed punctuations and special chars by keeping only alphanumerics\n",
    "                temp.append(data[i][j].lower())              # Converted words to lower case\n",
    "        TokenizeClean.append(temp)\n",
    "    return(TokenizeClean)\n",
    "\n",
    "def RemoveStopwords(data):\n",
    "    l3=list()\n",
    "    for i in list(range(0,len(data))):\n",
    "        temp=list()\n",
    "        for j in list(range(len(data[i]))):\n",
    "            if(data[i][j] not in stp_wrd): # Removed Stopwords from the nltk corpus\n",
    "                temp.append(data[i][j])\n",
    "        l3.append(temp)\n",
    "    return(l3)\n",
    "\n",
    "def TweetStemming(data):\n",
    "    from nltk.stem import PorterStemmer\n",
    "    ps=PorterStemmer()\n",
    "    l3=list()\n",
    "    for i in list(range(0,len(data))):\n",
    "        temp=list()\n",
    "        for j in list(range(len(data[i]))):\n",
    "            temp.append(ps.stem(data[i][j]))\n",
    "        l3.append(temp)\n",
    "    return(l3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "xtraintokenize=CleanTokenize(SentTokenize(l1))\n",
    "StemmedTweets=TweetStemming(RemoveStopwords(xtraintokenize))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TF/IDF Calculations------->>>>>>>>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#----Converting each tweet to String\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer  \n",
    "import pickle\n",
    "    \n",
    "TweetForIDF=list()\n",
    "str=\"\"\n",
    "for i in list(range(0,len(StemmedTweets))):\n",
    "    temp=list()\n",
    "    str=\"\"\n",
    "    str=(\" \".join(StemmedTweets[i]))\n",
    "    TweetForIDF.append(str)\n",
    "    \n",
    "## --- min_df=10 --The tweet words must occur in atleast 10 tweets in order to consider it as an important word for classification\n",
    "## --- max_df=0.7 -- If a word occurs in more than 70% of documents, that means it is not important for classification\n",
    "\n",
    "tfidfconverter = TfidfVectorizer(min_df=10, max_df=0.7, stop_words=stopwords.words('english'))  \n",
    "xtrain = tfidfconverter.fit_transform(TweetForIDF).toarray()\n",
    "pickle.dump(tfidfconverter, open('tfidfconverter.l', 'wb')) ##---- Storing the training corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##---- Processing Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "testdf = pd.read_csv('C:\\\\Python\\\\test_10.txt', header=None,names= [\"ItemID\", \"Sentiment\", \"SentimentSource\", \"SentimentText\"],\n",
    "                 skiprows=1) ##---- Input Test Data\n",
    "\n",
    "testtweet=testdf['SentimentText']\n",
    "ytest10=testdf['Sentiment']\n",
    "\n",
    "xtesttokenize=CleanTokenize(SentTokenize(testtweet))\n",
    "testtweetclean=TweetStemming(RemoveStopwords(xtesttokenize)) ###----Clean-Tokenized/Stemmed test data\n",
    "\n",
    "#----Converting each tweet to String\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer  \n",
    "import pickle\n",
    "    \n",
    "TweetForIDF=list()\n",
    "str=\"\"\n",
    "for i in list(range(0,len(testtweetclean))):\n",
    "    temp=list()\n",
    "    str=\"\"\n",
    "    str=(\" \".join(testtweetclean[i]))\n",
    "    TweetForIDF.append(str)\n",
    "    \n",
    "## --- min_df=10 --The tweet words must occur in atleast 10 tweets in order to consider it as an important word for classification\n",
    "## --- max_df=0.7 -- If a word occurs in more than 70% of documents, that means it is not important for classification\n",
    "\n",
    "tfidfconverter = pickle.load(open('tfidfconverter.l', 'rb')) ##---- Retrieving the corpus of training words\n",
    "xtest = tfidfconverter.transform(TweetForIDF).toarray()      ## --- TF/IDF of test data based on training corpus\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(89989, 4653)\n",
      "(10000, 4653)\n"
     ]
    }
   ],
   "source": [
    "print(xtrain.shape)\n",
    "print(xtest.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##----- Functions for Logistic Regression Model------>>>>>>>>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "##---- Sigmoid Function------\n",
    "\n",
    "def sigmoidfunction(x):\n",
    "    return (1/(1+np.exp(-x)))\n",
    "\n",
    "##--- Computing Theta.X\n",
    "def theta_x(theta,x):\n",
    "    return (np.dot(x,theta))\n",
    "\n",
    "##------ Probability function -----\n",
    "def retprob(theta,x):\n",
    "    return( sigmoidfunction(theta_x(theta,x)))\n",
    "\n",
    "##------ Logistic Loss Function\n",
    "def lossfunction(h,y):\n",
    "    return (-y * np.log(h) - (1 - y) * np.log(1 - h))\n",
    "\n",
    "##------ Calculating gradient for Logistic loss function\n",
    "def gradientcalc(x,y,h,theta,learningrate,numiters):\n",
    "    for i in range(0,numiters):\n",
    "        grad=np.dot(x.T,(h-y)) / y.shape[0]\n",
    "        thetanew=theta - learningrate*grad\n",
    "        theta=thetanew\n",
    "    return(thetanew)\n",
    "    \n",
    "##------ Predicting test/validation label\n",
    "def predictions(x,theta):\n",
    "    thet = theta[:, np.newaxis]\n",
    "    return retprob(thet, x)\n",
    "\n",
    "##--- Computing accuracy\n",
    "def accuracycalc(xtest,ytest,theta):\n",
    "    predictedclass=((predictions(xtest,theta)>=0.5) .astype(int))\n",
    "    predictedclass=predictedclass.flatten()\n",
    "    \n",
    "    accuracy=np.mean(predictedclass==ytest)\n",
    "    \n",
    "    return (accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cross Validation--------->>>>>>>>>>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pass Started---->\n",
      "Pass Finished---->\n",
      "Pass Started---->\n",
      "Pass Finished---->\n",
      "Pass Started---->\n",
      "Pass Finished---->\n",
      "Pass Started---->\n",
      "Pass Finished---->\n",
      "Pass Started---->\n",
      "Pass Finished---->\n",
      "Pass Started---->\n",
      "Pass Finished---->\n",
      "Pass Started---->\n",
      "Pass Finished---->\n",
      "Pass Started---->\n",
      "Pass Finished---->\n",
      "Pass Started---->\n",
      "Pass Finished---->\n",
      "Pass Started---->\n",
      "Pass Finished---->\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "\n",
    "kf = KFold(n_splits=10)         ###---- 10-fold cross validation\n",
    "accuracy=list()\n",
    "theta = np.zeros(xtrain.shape[1])\n",
    "thetabest=theta.shape\n",
    "bestaccuracy=0\n",
    "numiters=200\n",
    "\n",
    "for train, test in kf.split(xtrain):\n",
    "    \n",
    "    theta = np.zeros(xtrain.shape[1])\n",
    "    \n",
    "    xtraining=xtrain[train]\n",
    "    ytraining=ytrain[train]\n",
    "    xvalidate=xtrain[test]\n",
    "    yvalidate=ytrain[test]\n",
    "    \n",
    "    h=retprob(theta,xtraining)\n",
    "    print('Pass Started---->')\n",
    "    theta=gradientcalc(xtraining,ytraining,h,theta,0.1,numiters)\n",
    "    print('Pass Finished---->')\n",
    "    #predictedval=predictions(xtraining,theta)\n",
    "    \n",
    "    accuracyval=accuracycalc(xvalidate,yvalidate,theta)\n",
    "    \n",
    "    #---- Storing the accuracy and theta values based on best performance on validation set\n",
    "    \n",
    "    if(accuracyval>bestaccuracy):\n",
    "        thetabest=theta\n",
    "        bestaccuracy=accuracyval\n",
    "    \n",
    "    accuracy.append(accuracyval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7096"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##--- Accuracy on test data\n",
    "\n",
    "acc=accuracycalc(xtest,ytest10,thetabest)\n",
    "acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1897, 2109],\n",
       "       [ 847, 5147]], dtype=int64)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##--- Confusion Matrix -- Test Data\n",
    "\n",
    "testpredict=((predictions(xtest,thetabest)>0.5).astype(int)).flatten()\n",
    "from sklearn.metrics import confusion_matrix\n",
    "confusion_matrix(ytest10,testpredict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall-> 0.858692025358692\n",
      "Precision-> 0.7093439911797134\n"
     ]
    }
   ],
   "source": [
    "##-- Recall / Precision -- Test Data\n",
    "\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import precision_score\n",
    "\n",
    "print(\"Recall->\",recall_score(ytest10,testpredict))\n",
    "print(\"Precision->\",precision_score(ytest10,testpredict))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
